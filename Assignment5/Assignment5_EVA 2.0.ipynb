{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment5_EVA2.0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aNyZv-Ec52ot"
      },
      "source": [
        "Assignment 5\n",
        "============\n",
        "\n",
        "Name : Nihar Kanungo\n",
        "Batch : 6:30 AM , Monday\n",
        "\n",
        "\n",
        "**Background :**\n",
        "- - - - - - - -\n",
        "This is a simple Image Recognition program which makes use of the MNIST preprocessed dataset to process the handwritten digit images and predict the numerical digit each image resents . The Code uses one of the most popular Tensorflow API Keras to perform the operations .It's a supervised Computer Vision problem.\n",
        "\n",
        "There are 4 different networks defined in this file . Each network defined in this file is an improvement over it's Predecessor. \n",
        "\n",
        "Network -1 : This is the Basic Network which defines the template over which the improvements will be added. The Basic network here refers to the final code of Assignment -5 \n",
        "\n",
        "Network -2 : This Network works on the Normalized image after the images are standardized  Network-1\n",
        "\n",
        "Network -3 : This Network adds custom loss functin which is L2 regularization added to the cross entropy loss to penalize the bigger weights \n",
        "\n",
        "Network -4 : This Network reverses the order in which batch normalization and activation function was used. In this network we are using batch normalization before the activation function but after the convolution. That means the activation is currently happening external to the convolution function \n",
        "\n",
        "\n",
        "\n",
        "**Input**\n",
        "- - - -\n",
        "1) 60000 Handwritten digit images (between 0-9)\n",
        "\n",
        "2) The Images are already segreegated as Train and Test Data with the respective target values\n",
        "\n",
        "\n",
        "**Environment**\n",
        "- - - - - - - - \n",
        "\n",
        "    Development - Colab GPU , Jupyter Notebook\n",
        "    Repository : Github\n",
        "\n",
        "**Algorithm**\n",
        "- - - - - - \n",
        "    Linear Model \n",
        "    Convolutional Neural Network (2D) - Gray Scale images\n",
        "    Maxpooling \n",
        "    Softmax Activation function\n",
        "    loss Function : Categorical Crossentropy\n",
        "    Optimizer=Adam\n",
        "    Metrics=accuracy\n",
        "    Batch Normalization\n",
        "    Drop Out \n",
        "    Image Normalization\n",
        "    Customized Loss function (Loss function _ L2 Regularization)\n",
        "\n",
        "\n",
        "**Parameters**\n",
        "- - - - - - ---------------------\n",
        "\n",
        "    Batch Size - Variable \n",
        "    Epochs - Variable\n",
        "    Kernel Size - Variable (Advisable to use 3 * 3)\n",
        "    Number of Kernels - Variable \n",
        "    Learning Rate\n",
        "    regularization coefficient\n",
        "    \n",
        "\n",
        "**Conditions**\n",
        "- - - - - -\n",
        "\n",
        "1. The Number of parameters < 15,000\n",
        "2. Should use only Conv2D\n",
        "3. Should not have applied Maxpooling before 2-4 layers of the conversion into number of classes (10 in this case)\n",
        "4. Maxpooling should be applied on receptive field of at least 5 x 5 or 7 x 7\n",
        "5. Activation function should be relu on conv 2D\n",
        "6. With < 15 EPochs\n",
        "7. Image Normalization should be used first\n",
        "8. L2 regularization should not be added to each layer \n",
        "9. Batch Normalization should be used before the Activation function\n",
        "**Expected Result**\n",
        "\n",
        "- - - - - - - - -\n",
        "To get >= 99.4 % accuracy \n",
        "To find the first 25 misclassified images\n",
        "Save the Best model w.r.t the Validation accuracy\n",
        "Save the model in the Google drive for future use \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A_JGIXQ8EAO_"
      },
      "source": [
        "# **Import Libraries and modules**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3m3w1Cw49Zkt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca2f7b44-5f03-474f-8e46-cd1ca0cbe23a"
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Eso6UHE080D4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "c384c713-c8c6-4ddf-ae7a-c3543ba6dd9c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add,BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "from IPython.display import Image\n",
        "from keras.optimizers import Adam, SGD, Nadam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "import sys\n",
        "import dill\n",
        "import tensorflow as tf\n",
        "slim = tf.contrib.slim\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0819 17:18:04.678384 140531037792128 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zByEi95J86RD"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7eRM0QWN83PV",
        "outputId": "b435af19-40da-46a9-a7b4-6604b300dc39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hzKCdZmEAPM"
      },
      "source": [
        "# Print Metadata Information \n",
        "1. Print the shape of the Training Dataset (Number of Images, Size of the images)\n",
        "2. Import one of the popular library to plot charts/graphs and command to display it inline\n",
        "3. Code to show the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4a4Be72j8-ZC",
        "outputId": "0e2fbb8c-cdb8-4a6a-9948-ab0ad7e268c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "\n",
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf943f6c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WDO4QtQTEAPR"
      },
      "source": [
        "#Reshape the Training and test data to gray scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dkmprriw9AnZ",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PGJ9Z5rLEAPV"
      },
      "source": [
        "# Perform Data Standardization \n",
        "1. Convert the Images into float32 format\n",
        "2. Divide the values by 255 to make it with in 0-1 (Standardize)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X2m4YS4E9CRh",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzeZYL7TwwA9",
        "colab_type": "text"
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZG8JiXR39FHC",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AV85Am_ZEAPm"
      },
      "source": [
        "***The Core Setup for this assignment ( this is the final code of assignment No 4)***\n",
        "=====================\n",
        "This core setup is going to be the basic framework . All the future codes will be improvements over this code but the basic setup will be unaltered . This would help us to understand the impact of each feature on our network. The Core setup is described below.\n",
        "\n",
        "1. Import the Activation layer \n",
        "2. Declare the Sequential model\n",
        "3. Add multiple convolution layers to increase the receptive field ( 3 x 3 kernels)\n",
        "4. Add Maxpooling to reduce the image size and increase the receptive field \n",
        "5. Use small dropout after each convolution to ensure that the model is not over fitting\n",
        "6. Add Batch normalization after activation of each layer to essure that the model is generalizing well\n",
        "7.Modify the learning rate after each epoch to converge \n",
        "5. Add a large kernel of size 5 x 5 at this stage as going below this may not show significant details of the images\n",
        "6. Flatten the data to make it 1D \n",
        "7. Apply Softmax on each output to find out the predicted class\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lnDRKPvFEAPq"
      },
      "source": [
        "Display the Summary of the Model. This would give the detail parameters each layer uses . This would be a great information to find out how the memory will be used and where are oppertunities for fine tuning.\n",
        "\n",
        " The Total parameters used for this network is < 15000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9iSYhmXnJwRX",
        "outputId": "1c81e050-baa0-4a5d-b2a3-be4e513f0030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(14, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred[:9])\n",
        "print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_154 (Conv2D)          (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_103 (Bat (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_103 (Dropout)        (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_155 (Conv2D)          (None, 24, 24, 14)        1022      \n",
            "_________________________________________________________________\n",
            "batch_normalization_104 (Bat (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "dropout_104 (Dropout)        (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_156 (Conv2D)          (None, 22, 22, 24)        3048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_157 (Conv2D)          (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "conv2d_158 (Conv2D)          (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_105 (Bat (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_105 (Dropout)        (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_159 (Conv2D)          (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_106 (Bat (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_106 (Dropout)        (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_160 (Conv2D)          (None, 5, 5, 24)          3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "dropout_107 (Dropout)        (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_161 (Conv2D)          (None, 5, 5, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_108 (Bat (None, 5, 5, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_108 (Dropout)        (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_162 (Conv2D)          (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,430\n",
            "Trainable params: 14,254\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 35s 586us/step - loss: 0.1670 - acc: 0.9474 - val_loss: 0.0528 - val_acc: 0.9829\n",
            "Epoch 2/15\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 25s 420us/step - loss: 0.0664 - acc: 0.9794 - val_loss: 0.0453 - val_acc: 0.9862\n",
            "Epoch 3/15\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 25s 417us/step - loss: 0.0532 - acc: 0.9838 - val_loss: 0.0305 - val_acc: 0.9907\n",
            "Epoch 4/15\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0450 - acc: 0.9855 - val_loss: 0.0245 - val_acc: 0.9917\n",
            "Epoch 5/15\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 25s 415us/step - loss: 0.0382 - acc: 0.9875 - val_loss: 0.0275 - val_acc: 0.9913\n",
            "Epoch 6/15\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 25s 414us/step - loss: 0.0352 - acc: 0.9890 - val_loss: 0.0226 - val_acc: 0.9924\n",
            "Epoch 7/15\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 25s 413us/step - loss: 0.0315 - acc: 0.9899 - val_loss: 0.0263 - val_acc: 0.9907\n",
            "Epoch 8/15\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0293 - acc: 0.9905 - val_loss: 0.0273 - val_acc: 0.9910\n",
            "Epoch 9/15\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 25s 416us/step - loss: 0.0264 - acc: 0.9915 - val_loss: 0.0194 - val_acc: 0.9936\n",
            "Epoch 10/15\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 25s 412us/step - loss: 0.0252 - acc: 0.9917 - val_loss: 0.0220 - val_acc: 0.9928\n",
            "Epoch 11/15\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 25s 418us/step - loss: 0.0235 - acc: 0.9923 - val_loss: 0.0202 - val_acc: 0.9938\n",
            "Epoch 12/15\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 25s 411us/step - loss: 0.0224 - acc: 0.9928 - val_loss: 0.0212 - val_acc: 0.9939\n",
            "Epoch 13/15\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 24s 408us/step - loss: 0.0216 - acc: 0.9930 - val_loss: 0.0208 - val_acc: 0.9931\n",
            "Epoch 14/15\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0203 - acc: 0.9936 - val_loss: 0.0224 - val_acc: 0.9935\n",
            "Epoch 15/15\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 25s 410us/step - loss: 0.0198 - acc: 0.9932 - val_loss: 0.0217 - val_acc: 0.9935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa1f919df60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        },
        {
          "output_type": "stream",
          "text": [
            "[0.021683135201784172, 0.9935]\n",
            "[[9.18202320e-12 4.11605328e-08 2.96076035e-08 2.18741363e-07\n",
            "  6.37922781e-08 2.65363093e-10 1.05903741e-12 9.99999046e-01\n",
            "  7.02135711e-11 6.27163956e-07]\n",
            " [1.02429249e-06 8.89295450e-07 9.99997377e-01 6.62932376e-09\n",
            "  2.20760811e-07 1.34024606e-12 3.47157510e-07 1.91765004e-09\n",
            "  6.49364367e-08 3.06864117e-10]\n",
            " [8.74688055e-08 9.99996185e-01 1.13632382e-07 2.02231703e-07\n",
            "  2.13324640e-07 2.87845296e-07 1.20585742e-06 1.48125787e-06\n",
            "  4.75331099e-08 1.40844083e-08]\n",
            " [9.99311328e-01 9.80048889e-11 1.34738873e-10 5.04616651e-08\n",
            "  6.14594731e-09 3.53703683e-07 6.86454761e-04 3.09342507e-09\n",
            "  5.02739226e-07 1.45698175e-06]\n",
            " [1.31976108e-09 2.90482705e-09 8.89440521e-10 3.39934331e-11\n",
            "  9.99985576e-01 1.42430095e-10 1.87326998e-09 7.39263717e-09\n",
            "  1.34966868e-07 1.42858698e-05]\n",
            " [5.87095244e-08 9.99988079e-01 1.61987060e-07 5.44403314e-08\n",
            "  8.42517579e-07 1.67542815e-08 2.98381423e-07 1.04503561e-05\n",
            "  5.54504709e-08 7.26151868e-08]\n",
            " [3.77705171e-14 3.41752127e-07 2.61288280e-09 8.65042649e-10\n",
            "  9.99910355e-01 3.33925199e-09 9.76861717e-11 6.02590408e-05\n",
            "  5.78013896e-06 2.31744634e-05]\n",
            " [7.32903374e-08 3.73751652e-09 2.98139696e-07 3.83453471e-06\n",
            "  2.00932682e-05 1.74047125e-06 9.20894827e-09 3.58003803e-07\n",
            "  4.62991864e-07 9.99973059e-01]\n",
            " [1.91672705e-04 1.44308965e-08 1.58858597e-08 1.78564648e-07\n",
            "  5.95954974e-09 9.58953857e-01 4.08407226e-02 2.22735452e-09\n",
            "  8.79424442e-06 4.72121110e-06]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6m9-B2XMcb3F"
      },
      "source": [
        "Observations\n",
        "---------------------\n",
        "\n",
        "\n",
        "At the first look the results looks good. But did you also observe that we achieved the magic number as well ? If you haven't then go back and see that we achieved it in the 12th Epoch and improved over it on the 15th one . We can also try to see if more number of epochs takes it further up. But before that let's try and focus on the charts to see how it improved over different epochs and how the loss did .\n",
        "\n",
        "\n",
        "\n",
        "This shows that \n",
        "\n",
        "1. The validation loss was always better than the Training loss and \n",
        "2. Simillarly the validation accuracy was always higher than the training accuracy\n",
        "3. The network also narrowed down the difference between the training and validation accuracies \n",
        "\n",
        "That means it did a great job in generalizing the data . We achieved the target with \n",
        "\n",
        "1. Less than 15000 Parameters \n",
        "2. Less than 15 epochs\n",
        "3. WIth no greater than 32 batch size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZlkaMC5oj3u",
        "colab_type": "text"
      },
      "source": [
        "# Define Custom Loss function \n",
        "\n",
        "Below we have defined a method which takes two values  i) the actual values and ii) the predicted value\n",
        "\n",
        "Definee the L2 Regression function\n",
        "\n",
        "Define the value of lambda ( configurable)\n",
        "\n",
        "Calculated the sum of the square of all the weights of all the layers \n",
        "\n",
        "add the L2 Regression value to the Cross entropy loss\n",
        "\n",
        "Returns the value of new loss function which will be used during model fitting and validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT5g4CfaPTMt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NZeuvkToeoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define custom loss\n",
        "\n",
        "def custom_loss(actual,predicted):\n",
        "    print(actual)\n",
        "    print(predicted)\n",
        "    \n",
        "    sqr_w = 0\n",
        "    lamda = 0.001 #1e-4\n",
        "\n",
        "    for layer in model.layers:\n",
        "       sqr_w = sqr_w + np.sum(np.sum(np.sum(np.square(layer.get_weights()))))\n",
        "\n",
        "    l2_regularization = (lamda*sqr_w)/2*(32) \n",
        "    loss = K.categorical_crossentropy(actual,predicted) + l2_regularization\n",
        "\n",
        "    # Return a function\n",
        "    return loss\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QexwfPcCPY7p",
        "colab_type": "text"
      },
      "source": [
        "# Connection to Google Drive for saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmcNkw6Tl8fD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "843eb49c-26eb-44bb-d3ea-09f4438ff31b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji0PLYT0PhnT",
        "colab_type": "text"
      },
      "source": [
        "# Change the root path (if required)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UQ1TgfDmvxC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33cc515a-e9c0-4e47-fe1f-710b5bbbcc6c"
      },
      "source": [
        "\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/'  #change dir to your project folder"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kZcs9VePlfC",
        "colab_type": "text"
      },
      "source": [
        "# This model is using Image normalization\n",
        "\n",
        "1) Creates an image Data Generator \n",
        "\n",
        "2) Fit the model using the Data Generator using a batch size of 512 for 40 epochs \n",
        "\n",
        "3) Saves the Model with highest validation accuracy to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sg33QGHvdLf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c023f3c-f329-44dc-fe40-90c414d9700d"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(14, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "es = EarlyStopping(\"val_acc\",patience=15,restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "callbacks_list = [es,checkpoint,LearningRateScheduler(scheduler, verbose=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "model.compile(loss=custom_loss, optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True,)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=512,shuffle=True),\n",
        "                    steps_per_epoch=int(np.ceil(len(X_train)/512)), \n",
        "                    epochs=40, verbose=1, validation_data=datagen.flow(X_test, Y_test, batch_size=512,shuffle=True), \n",
        "                    validation_steps = int(np.ceil(len(X_test)/512)), \n",
        "                    callbacks=callbacks_list,)\n",
        "\n",
        "\n",
        "\n",
        "#model.fit(X_train, Y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "\n",
        "model.load_weights(\"model.hdf5\")\n",
        "\n",
        "# steps=np.ceil(len(X_train)/1024)\n",
        "# since it is auto calculated by kera so don't fill  it\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# #_ = predictions(score, predictions,Y_train, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# iterator = datagen.flow(X_test, Y_test, batch_size=512, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "iterator = datagen.flow(X_test, Y_test, batch_size=512,shuffle=False)\n",
        "predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "predicted_class_indices=np.argmax(predictions,axis=1)\n",
        "print(predicted_class_indices[0])\n",
        "\n",
        "#_ = predictions(score, predictions,Y_test, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# _=evaluate(model,X_train, Y_train,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "# _=evaluate(model,X_test, Y_test,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "# print(score)\n",
        "\n",
        "\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(y_pred[:9])\n",
        "# print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0819 12:16:55.949583 140469947746176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0819 12:16:55.959606 140469947746176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0819 12:16:55.972234 140469947746176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0819 12:16:56.022757 140469947746176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0819 12:16:56.024510 140469947746176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0819 12:16:58.953382 140469947746176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0819 12:16:59.049420 140469947746176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0819 12:16:59.204262 140469947746176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0819 12:16:59.772166 140469947746176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 14)        1022      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 24)        3048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 5, 5, 24)          3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 5, 5, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 5, 5, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,430\n",
            "Trainable params: 14,254\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n",
            "Tensor(\"activation_1_target:0\", shape=(?, ?), dtype=float32)\n",
            "Tensor(\"activation_1/Softmax:0\", shape=(?, ?), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0819 12:17:00.754199 140469947746176 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "118/118 [==============================] - 10s 85ms/step - loss: 46.7384 - acc: 0.8676 - val_loss: 46.4150 - val_acc: 0.9706\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "118/118 [==============================] - 6s 49ms/step - loss: 46.4122 - acc: 0.9719 - val_loss: 46.3778 - val_acc: 0.9797\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3883 - acc: 0.9788 - val_loss: 46.3608 - val_acc: 0.9868\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3777 - acc: 0.9818 - val_loss: 46.3557 - val_acc: 0.9882\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "118/118 [==============================] - 6s 49ms/step - loss: 46.3681 - acc: 0.9849 - val_loss: 46.3504 - val_acc: 0.9900\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3650 - acc: 0.9855 - val_loss: 46.3473 - val_acc: 0.9914\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "118/118 [==============================] - 6s 49ms/step - loss: 46.3614 - acc: 0.9871 - val_loss: 46.3536 - val_acc: 0.9895\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "118/118 [==============================] - 6s 49ms/step - loss: 46.3587 - acc: 0.9871 - val_loss: 46.3499 - val_acc: 0.9897\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 46.3554 - acc: 0.9885 - val_loss: 46.3432 - val_acc: 0.9928\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "118/118 [==============================] - 6s 49ms/step - loss: 46.3530 - acc: 0.9895 - val_loss: 46.3443 - val_acc: 0.9923\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 46.3523 - acc: 0.9897 - val_loss: 46.3427 - val_acc: 0.9925\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3526 - acc: 0.9895 - val_loss: 46.3438 - val_acc: 0.9925\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3511 - acc: 0.9901 - val_loss: 46.3446 - val_acc: 0.9929\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3495 - acc: 0.9906 - val_loss: 46.3446 - val_acc: 0.9925\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3483 - acc: 0.9911 - val_loss: 46.3443 - val_acc: 0.9921\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3477 - acc: 0.9910 - val_loss: 46.3417 - val_acc: 0.9930\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3460 - acc: 0.9916 - val_loss: 46.3486 - val_acc: 0.9913\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3461 - acc: 0.9914 - val_loss: 46.3427 - val_acc: 0.9932\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3451 - acc: 0.9919 - val_loss: 46.3410 - val_acc: 0.9930\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3456 - acc: 0.9915 - val_loss: 46.3427 - val_acc: 0.9934\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3449 - acc: 0.9918 - val_loss: 46.3428 - val_acc: 0.9925\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3436 - acc: 0.9922 - val_loss: 46.3402 - val_acc: 0.9934\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3430 - acc: 0.9925 - val_loss: 46.3415 - val_acc: 0.9935\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 46.3439 - acc: 0.9920 - val_loss: 46.3416 - val_acc: 0.9937\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3421 - acc: 0.9926 - val_loss: 46.3400 - val_acc: 0.9937\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3417 - acc: 0.9926 - val_loss: 46.3447 - val_acc: 0.9931\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3411 - acc: 0.9928 - val_loss: 46.3396 - val_acc: 0.9943\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 46.3402 - acc: 0.9933 - val_loss: 46.3395 - val_acc: 0.9938\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3405 - acc: 0.9935 - val_loss: 46.3403 - val_acc: 0.9937\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3403 - acc: 0.9933 - val_loss: 46.3428 - val_acc: 0.9928\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3412 - acc: 0.9928 - val_loss: 46.3413 - val_acc: 0.9936\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3398 - acc: 0.9935 - val_loss: 46.3404 - val_acc: 0.9936\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3404 - acc: 0.9931 - val_loss: 46.3387 - val_acc: 0.9945\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3388 - acc: 0.9937 - val_loss: 46.3401 - val_acc: 0.9935\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3399 - acc: 0.9931 - val_loss: 46.3392 - val_acc: 0.9938\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3389 - acc: 0.9935 - val_loss: 46.3393 - val_acc: 0.9936\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3391 - acc: 0.9936 - val_loss: 46.3404 - val_acc: 0.9935\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3389 - acc: 0.9937 - val_loss: 46.3393 - val_acc: 0.9941\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 46.3376 - acc: 0.9939 - val_loss: 46.3406 - val_acc: 0.9935\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 46.3372 - acc: 0.9941 - val_loss: 46.3397 - val_acc: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc10c67be80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPsp6u7oQDoV",
        "colab_type": "text"
      },
      "source": [
        "# Adding the L2 Regularization with the Loss function\n",
        "\n",
        "The below model is trained using the custom loss function which is nothing but the combination of cross entropy loss and L2 regularization \n",
        "The model that gave the highest accuracy is saved in the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1DdIKu-R_XF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "222ac255-eeb8-4ba4-97ac-ee1cfed6f044"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        " \n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(14, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(8, 1, 1, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(24, 3, 3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 1, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "es = EarlyStopping(\"val_acc\",patience=15,restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "callbacks_list = [es,checkpoint,LearningRateScheduler(scheduler, verbose=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "model.compile(loss=custom_loss, optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True,)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=512,shuffle=True),\n",
        "                    steps_per_epoch=int(np.ceil(len(X_train)/512)), \n",
        "                    epochs=40, verbose=1, validation_data=datagen.flow(X_test, Y_test, batch_size=512,shuffle=True), \n",
        "                    validation_steps = int(np.ceil(len(X_test)/512)), \n",
        "                    callbacks=callbacks_list,)\n",
        "\n",
        "\n",
        "\n",
        "#model.fit(X_train, Y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "\n",
        "model.load_weights(\"model.hdf5\")\n",
        "\n",
        "# steps=np.ceil(len(X_train)/1024)\n",
        "# since it is auto calculated by kera so don't fill  it\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# #_ = predictions(score, predictions,Y_train, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# iterator = datagen.flow(X_test, Y_test, batch_size=512, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "iterator = datagen.flow(X_test, Y_test, batch_size=512,shuffle=False)\n",
        "predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "predicted_class_indices=np.argmax(predictions,axis=1)\n",
        "print(predicted_class_indices[0])\n",
        "\n",
        "#_ = predictions(score, predictions,Y_test, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# _=evaluate(model,X_train, Y_train,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "# _=evaluate(model,X_test, Y_test,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "# print(score)\n",
        "\n",
        "\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(y_pred[:9])\n",
        "# print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 24, 24, 14)        1022      \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 22, 22, 24)        3048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 5, 5, 24)          3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 5, 5, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 5, 5, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,430\n",
            "Trainable params: 14,254\n",
            "Non-trainable params: 176\n",
            "_________________________________________________________________\n",
            "Tensor(\"activation_3_target:0\", shape=(?, ?), dtype=float32)\n",
            "Tensor(\"activation_3/Softmax:0\", shape=(?, ?), dtype=float32)\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "118/118 [==============================] - 9s 75ms/step - loss: 5.0574 - acc: 0.8617 - val_loss: 4.7348 - val_acc: 0.9646\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.7189 - acc: 0.9699 - val_loss: 4.6789 - val_acc: 0.9822\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 4.6904 - acc: 0.9784 - val_loss: 4.6636 - val_acc: 0.9862\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6788 - acc: 0.9822 - val_loss: 4.6635 - val_acc: 0.9867\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6725 - acc: 0.9837 - val_loss: 4.6564 - val_acc: 0.9885\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6669 - acc: 0.9861 - val_loss: 4.6496 - val_acc: 0.9907\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6629 - acc: 0.9868 - val_loss: 4.6502 - val_acc: 0.9904\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6589 - acc: 0.9883 - val_loss: 4.6482 - val_acc: 0.9910\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6581 - acc: 0.9883 - val_loss: 4.6468 - val_acc: 0.9911\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6560 - acc: 0.9894 - val_loss: 4.6465 - val_acc: 0.9914\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "118/118 [==============================] - 6s 53ms/step - loss: 4.6535 - acc: 0.9898 - val_loss: 4.6478 - val_acc: 0.9908\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6535 - acc: 0.9899 - val_loss: 4.6457 - val_acc: 0.9915\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6507 - acc: 0.9909 - val_loss: 4.6464 - val_acc: 0.9913\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6504 - acc: 0.9908 - val_loss: 4.6447 - val_acc: 0.9916\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "118/118 [==============================] - 6s 52ms/step - loss: 4.6500 - acc: 0.9910 - val_loss: 4.6451 - val_acc: 0.9920\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6491 - acc: 0.9912 - val_loss: 4.6450 - val_acc: 0.9925\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6484 - acc: 0.9917 - val_loss: 4.6449 - val_acc: 0.9925\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6470 - acc: 0.9922 - val_loss: 4.6435 - val_acc: 0.9920\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6455 - acc: 0.9923 - val_loss: 4.6431 - val_acc: 0.9931\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6451 - acc: 0.9921 - val_loss: 4.6471 - val_acc: 0.9921\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6453 - acc: 0.9927 - val_loss: 4.6437 - val_acc: 0.9931\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6457 - acc: 0.9925 - val_loss: 4.6428 - val_acc: 0.9935\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6445 - acc: 0.9926 - val_loss: 4.6431 - val_acc: 0.9932\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6436 - acc: 0.9927 - val_loss: 4.6429 - val_acc: 0.9930\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6433 - acc: 0.9933 - val_loss: 4.6440 - val_acc: 0.9925\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6433 - acc: 0.9927 - val_loss: 4.6408 - val_acc: 0.9941\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6429 - acc: 0.9932 - val_loss: 4.6434 - val_acc: 0.9935\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6429 - acc: 0.9933 - val_loss: 4.6425 - val_acc: 0.9942\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6421 - acc: 0.9933 - val_loss: 4.6436 - val_acc: 0.9933\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6414 - acc: 0.9935 - val_loss: 4.6422 - val_acc: 0.9935\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6411 - acc: 0.9935 - val_loss: 4.6414 - val_acc: 0.9934\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6410 - acc: 0.9935 - val_loss: 4.6421 - val_acc: 0.9940\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6401 - acc: 0.9940 - val_loss: 4.6421 - val_acc: 0.9932\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6404 - acc: 0.9940 - val_loss: 4.6436 - val_acc: 0.9932\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6402 - acc: 0.9939 - val_loss: 4.6409 - val_acc: 0.9940\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6395 - acc: 0.9944 - val_loss: 4.6417 - val_acc: 0.9934\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6404 - acc: 0.9940 - val_loss: 4.6403 - val_acc: 0.9942\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6392 - acc: 0.9941 - val_loss: 4.6420 - val_acc: 0.9941\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "118/118 [==============================] - 6s 51ms/step - loss: 4.6396 - acc: 0.9940 - val_loss: 4.6408 - val_acc: 0.9933\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "118/118 [==============================] - 6s 50ms/step - loss: 4.6397 - acc: 0.9942 - val_loss: 4.6424 - val_acc: 0.9942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc15a8e4e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dftPBbBJQLGP",
        "colab_type": "text"
      },
      "source": [
        "# By Adding Batch Normalization Before Relu\n",
        "\n",
        "The model performs the following tasks \n",
        "\n",
        "1. Takes the activation layer outside of the convolution function\n",
        "2. Place Batch normalization function before the activation \n",
        "3. Trains the model for 40 epochs\n",
        "4. Saves the model with highest validation accuracy\n",
        "5. Saves the Model to google drive for future use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPRpY3ZsQPxw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b4f63ab-2a04-43db-8aa9-689ae14438d5"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "#model.add(BatchNormalization()) \n",
        "model.add(Convolution2D(8, 3, 3, input_shape=(28,28,1)))\n",
        "\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(14, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.1))\n",
        "                        \n",
        "\n",
        "\n",
        "model.add(Convolution2D(24, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Convolution2D(8, 1, 1))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))                        \n",
        "model.add(Dropout(0.1))\n",
        "                        \n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))                          \n",
        "model.add(Dropout(0.1))\n",
        "                        \n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(24, 3, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))                        \n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Convolution2D(10, 1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))                        \n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, 5))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "es = EarlyStopping(\"val_acc\",patience=15,restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint(\"model.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
        "callbacks_list = [es,checkpoint,LearningRateScheduler(scheduler, verbose=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "model.compile(loss=custom_loss, optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True,)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=512,shuffle=True),\n",
        "                    steps_per_epoch=int(np.ceil(len(X_train)/512)), \n",
        "                    epochs=40, verbose=1, validation_data=datagen.flow(X_test, Y_test, batch_size=512,shuffle=True), \n",
        "                    validation_steps = int(np.ceil(len(X_test)/512)), \n",
        "                    callbacks=callbacks_list,)\n",
        "\n",
        "\n",
        "\n",
        "#model.fit(X_train, Y_train, batch_size=32, epochs=15, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "\n",
        "model.load_weights(\"model.hdf5\")\n",
        "\n",
        "# steps=np.ceil(len(X_train)/1024)\n",
        "# since it is auto calculated by kera so don't fill  it\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# iterator = datagen.flow(X_train, Y_train, batch_size=1024, shuffle=False)\n",
        "# predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "\n",
        "# #_ = predictions(score, predictions,Y_train, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# iterator = datagen.flow(X_test, Y_test, batch_size=512, shuffle=False)\n",
        "# score = model.evaluate_generator(iterator,steps=len(iterator))\n",
        "\n",
        "iterator = datagen.flow(X_test, Y_test, batch_size=512,shuffle=False)\n",
        "predictions = model.predict_generator(iterator,steps=len(iterator))\n",
        "predicted_class_indices=np.argmax(predictions,axis=1)\n",
        "print(predicted_class_indices[0])\n",
        "\n",
        "#_ = predictions(score, predictions,Y_test, classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "# _=evaluate(model,X_train, Y_train,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "# _=evaluate(model,X_test, Y_test,classes=get_mnist_labels(),print_results=True, plot_results=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "# print(score)\n",
        "\n",
        "\n",
        "# y_pred = model.predict(X_test)\n",
        "# print(y_pred[:9])\n",
        "# print(y_test[:9])\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0819 17:19:29.726257 140531037792128 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 24, 24, 14)        1022      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 24, 24, 14)        56        \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 24, 24, 14)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 22, 22, 24)        3048      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 22, 22, 24)        96        \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 22, 22, 24)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 11, 11, 24)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 11, 11, 8)         200       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 11, 11, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 9, 9, 16)          1168      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 7, 7, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 5, 5, 24)          3480      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 5, 5, 24)          96        \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 5, 5, 24)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 5, 5, 10)          250       \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 5, 5, 10)          40        \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 5, 5, 10)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 1, 1, 10)          2510      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 14,526\n",
            "Trainable params: 14,302\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n",
            "Tensor(\"activation_18_target:0\", shape=(?, ?), dtype=float32)\n",
            "Tensor(\"activation_18/Softmax:0\", shape=(?, ?), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0819 17:19:30.968930 140531037792128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "118/118 [==============================] - 8s 70ms/step - loss: 5.8127 - acc: 0.8643 - val_loss: 5.5670 - val_acc: 0.9399\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.4635 - acc: 0.9697 - val_loss: 5.4325 - val_acc: 0.9794\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "118/118 [==============================] - 3s 30ms/step - loss: 5.4377 - acc: 0.9773 - val_loss: 5.4147 - val_acc: 0.9834\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.4263 - acc: 0.9808 - val_loss: 5.4016 - val_acc: 0.9892\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.4165 - acc: 0.9836 - val_loss: 5.4008 - val_acc: 0.9878\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.4109 - acc: 0.9855 - val_loss: 5.3920 - val_acc: 0.9915\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.4075 - acc: 0.9864 - val_loss: 5.3925 - val_acc: 0.9907\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.4063 - acc: 0.9866 - val_loss: 5.3907 - val_acc: 0.9918\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.4017 - acc: 0.9879 - val_loss: 5.3890 - val_acc: 0.9921\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.4007 - acc: 0.9884 - val_loss: 5.3871 - val_acc: 0.9920\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3993 - acc: 0.9890 - val_loss: 5.3874 - val_acc: 0.9923\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3975 - acc: 0.9895 - val_loss: 5.3858 - val_acc: 0.9924\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3962 - acc: 0.9900 - val_loss: 5.3867 - val_acc: 0.9928\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3955 - acc: 0.9899 - val_loss: 5.3857 - val_acc: 0.9930\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "118/118 [==============================] - 4s 30ms/step - loss: 5.3945 - acc: 0.9903 - val_loss: 5.3873 - val_acc: 0.9924\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "118/118 [==============================] - 4s 30ms/step - loss: 5.3939 - acc: 0.9903 - val_loss: 5.3854 - val_acc: 0.9923\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "118/118 [==============================] - 4s 30ms/step - loss: 5.3927 - acc: 0.9908 - val_loss: 5.3848 - val_acc: 0.9933\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3924 - acc: 0.9911 - val_loss: 5.3852 - val_acc: 0.9932\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3914 - acc: 0.9914 - val_loss: 5.3840 - val_acc: 0.9933\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "118/118 [==============================] - 3s 30ms/step - loss: 5.3935 - acc: 0.9906 - val_loss: 5.3850 - val_acc: 0.9938\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3902 - acc: 0.9915 - val_loss: 5.3848 - val_acc: 0.9935\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "118/118 [==============================] - 3s 30ms/step - loss: 5.3903 - acc: 0.9911 - val_loss: 5.3846 - val_acc: 0.9931\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "118/118 [==============================] - 3s 30ms/step - loss: 5.3896 - acc: 0.9916 - val_loss: 5.3849 - val_acc: 0.9934\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3889 - acc: 0.9920 - val_loss: 5.3860 - val_acc: 0.9933\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3902 - acc: 0.9920 - val_loss: 5.3840 - val_acc: 0.9936\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3892 - acc: 0.9916 - val_loss: 5.3849 - val_acc: 0.9933\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3879 - acc: 0.9924 - val_loss: 5.3847 - val_acc: 0.9937\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3885 - acc: 0.9923 - val_loss: 5.3850 - val_acc: 0.9938\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3879 - acc: 0.9922 - val_loss: 5.3845 - val_acc: 0.9935\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3873 - acc: 0.9922 - val_loss: 5.3834 - val_acc: 0.9938\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3873 - acc: 0.9926 - val_loss: 5.3844 - val_acc: 0.9935\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3865 - acc: 0.9928 - val_loss: 5.3836 - val_acc: 0.9933\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "118/118 [==============================] - 3s 30ms/step - loss: 5.3870 - acc: 0.9924 - val_loss: 5.3845 - val_acc: 0.9936\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3869 - acc: 0.9929 - val_loss: 5.3846 - val_acc: 0.9939\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3862 - acc: 0.9927 - val_loss: 5.3829 - val_acc: 0.9940\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3870 - acc: 0.9923 - val_loss: 5.3843 - val_acc: 0.9935\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3865 - acc: 0.9926 - val_loss: 5.3835 - val_acc: 0.9939\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3866 - acc: 0.9929 - val_loss: 5.3831 - val_acc: 0.9941\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3858 - acc: 0.9932 - val_loss: 5.3832 - val_acc: 0.9937\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "118/118 [==============================] - 3s 29ms/step - loss: 5.3856 - acc: 0.9931 - val_loss: 5.3841 - val_acc: 0.9941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcf37107a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOh-apwDQRSI",
        "colab_type": "text"
      },
      "source": [
        "# Save the Model to Google Drive for Future Use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO1iHRTMzlPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('gdrive/My Drive/Colab Notebooks/model_3.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-yXLCp9QUnb",
        "colab_type": "text"
      },
      "source": [
        "# Display the Misclassified Images (25)\n",
        "\n",
        "Display the first 25 misclassified images\n",
        "\n",
        "The image should include the Predicted Class and Actual Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSF-Kvlt3eoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8h4Mto8ANf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "outputId": "896b3bc0-6ac5-4d8b-f387-54d0817551b3"
      },
      "source": [
        "%matplotlib inline\n",
        "#y_pred = model.predict_classes(X_test)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28)\n",
        "image_cnt = 0\n",
        "#display_cnt=1\n",
        "w=10\n",
        "h=10\n",
        "fig=plt.figure(figsize=(12, 12))\n",
        "columns = 5\n",
        "rows = 5\n",
        "\n",
        "for i in range(len(predicted_class_indices)):\n",
        " # print('value of i before is',i)\n",
        "  if predicted_class_indices[i] != y_test[i]:\n",
        "    ax = fig.add_subplot(rows, columns, image_cnt+1)\n",
        "    title = 'Pred-' + str(predicted_class_indices[i]) +'Actual-' + str(y_test[i] )\n",
        "   # ax.title.set_text('                      ' )\n",
        "    ax.title.set_text(title )\n",
        "    \n",
        "    plt.imshow(X_test[i])\n",
        "    image_cnt +=1\n",
        "    #display_cnt +=1\n",
        "   # print('value of image count is', image_cnt)  \n",
        "  if image_cnt >= 25:\n",
        "    break\n",
        "    i = len(y_pred)\n",
        "   # print(' length of ypred is',len(y_pred),'and value of i now is',i)\n",
        "\n",
        "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
        "plt.show()    \n",
        "\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bdabb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bce2748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bc89e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bcbb6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bc64ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bc13748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bbbdf98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bbec7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bb94fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bb45828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bb79080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bb1f898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2bad00f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2baf9908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2baac160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2ba52978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2ba051d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2ba2d9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2b9dd240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2b987a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2b9b72b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2b95dac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2b911320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2b939b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcf2b8ea390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAK7CAYAAAD8yjntAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n36AWCXUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x864 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTk8juueQcXY",
        "colab_type": "text"
      },
      "source": [
        "# Find out the Total number of misclassified images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riY37-K0Jjgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd1f7c18-8dad-43fb-f57a-79e8df405c16"
      },
      "source": [
        "#y_pred = model.predict_classes(X_test)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28)\n",
        "count1 =0\n",
        "for i in range(len(predicted_class_indices)):\n",
        " # print('value of i before is',i)\n",
        "  if predicted_class_indices[i] != y_test[i]:\n",
        "    count1 +=1\n",
        "print('count is', count1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count is 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9tpU-DKOIbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}