{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the pybullet environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "Ls4Pfvg1eCUr",
    "outputId": "b726542e-1a09-4b28-c6f1-82a12d812c78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pybullet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/ac/a422ab8d1c57ab3f43e573b5a5f532e6afd348d81308fe66a1ecb691548e/pybullet-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (95.0MB)\n",
      "\u001b[K     |████████████████████████████████| 95.0MB 55kB/s \n",
      "\u001b[?25hInstalling collected packages: pybullet\n",
      "Successfully installed pybullet-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pybullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries \n",
    "\n",
    "1.Pybullet is an environment which is a plugin for gym which allows us to do some sort of 3d animation \n",
    "\n",
    "2.Wrapper is used to wrap the pybullet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3xrwYembTAU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import torch.nn as nn\n",
    "from gym import wrappers\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34ToooBqEY9V"
   },
   "source": [
    "## Step 1\n",
    "\n",
    "\n",
    "1.We initialize the Experience Replay Memory with the size of 1e6.\n",
    "\n",
    "2. Then we populate it with new transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXgDYV4Jb3ip"
   },
   "outputs": [],
   "source": [
    "# Defines the ReplayBuffer Class so that we can create objects of this class \n",
    "class ReplayBuffer(object):\n",
    "# Initialization Class \n",
    "  def __init__(self,max_size=1e6):\n",
    "    self.storage=[]\n",
    "    self.max_size=max_size\n",
    "    self.ptr=0\n",
    "#***********************************************************************************************************************\n",
    "# This function is the add transition function which adds the trasnsitions to the Replaybuffer\n",
    "#     1. If the length of the storage is equal to the maximum size defined for the replay buffer then the pointer\n",
    "#        is reset to the first location and starts overwritting the new records with old saved ones sequentially\n",
    "#     2. if not then the transactions are appended to the storage (Replay buffer)\n",
    "#***********************************************************************************************************************\n",
    "  def add(self,transition):\n",
    "    if len(self.storage)== self.max_size:\n",
    "      self.storage[int(self.ptr)]=transition\n",
    "      self.ptr=(self.ptr+1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)  \n",
    "#**********************************************************************************************************************\n",
    "# Ths function returns a random subset of Replay memory based on the given batch size \n",
    "#    1.Define empty lists to store current state, next state , actions , rewards and done \n",
    "#    2.Read each of the above sequentially from the stoarge \n",
    "#    3. Append the fetched data into the respective lists \n",
    "#    4.Return the data to the calling function \n",
    "#**********************************************************************************************************************\n",
    "  def sample(self,batch_size):\n",
    "    ind=np.random.randint(0,len(self.storage),batch_size)\n",
    "    batch_state,batch_next_state,batch_action,batch_reward,batch_dones=[],[],[],[],[]\n",
    "    for i in ind:\n",
    "      state,next_state,action,reward,done=self.storage[i]\n",
    "      batch_state.append(np.array(state,copy=False))\n",
    "      batch_next_state.append(np.append(next_state,copy=False))\n",
    "      batch_action.append(np.append(action,copy=False))\n",
    "      batch_reward.append(np.append(reward,copy=False))\n",
    "      batch_dones.append(np.append(done,copy=False))\n",
    "    return np.array(batch_state),np.array(batch_next_state),np.array(batch_action),np.array(batch_reward).reshape(-1,1),np.array(batch_dones).reshape(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DxmxOh-EiHZ"
   },
   "source": [
    "## Step 2\n",
    "\n",
    "Build a Model for Actor MOdel and Actor Target. As we are building the same model for both hence the definition are also the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_IZh8q4jTs8"
   },
   "outputs": [],
   "source": [
    "#**************************************************************************************************************************\n",
    "#It takes the input as the \n",
    "1. State dims  - State parameters \n",
    "2. action_dims - How many actions can be taken\n",
    "3. max_action  - the limit that each action can take (for example 5 degree, 10 degree etc.)\n",
    "#**************************************************************************************************************************\n",
    "\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self,state_dims,action_dims,max_action):\n",
    "    super(Actor,self).__init__()\n",
    "    self.layer1=nn.Linear(state_dims,400)\n",
    "    self.layer2=nn.Linear(400,300)\n",
    "    self.layer3=nn.Linear(300,action_dims)\n",
    "    self.max_action=max_action\n",
    "#**************************************************************************************************************************\n",
    "# The point here is to see the third line which takes the x value and apply the tanh function to keep it between \n",
    "# -1 and +1 and then multiplying that with the max action which helps them to take exactly the same angle,value \n",
    "#**************************************************************************************************************************\n",
    "  def forward(self,x):\n",
    "    x=F.relu(self.layer1(x))\n",
    "    x=F.relu(self.layer2(x))\n",
    "    x=self.max_action*torch.tanh(self.layer3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4J-O97ffEkmR"
   },
   "source": [
    "## Step 3\n",
    "\n",
    "We are creating 2 Critic models here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UijkgNgklnhV"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "#**************************************************************************************************************************\n",
    "# It takes the input as the \n",
    "# 1. State dims  - State parameters \n",
    "# 2. action_dims - How many actions can be taken\n",
    "# 3. max_action  - Not required as it's going to come from \n",
    "# We have 2 critic here as the dimensions are different\n",
    "#**************************************************************************************************************************    \n",
    "  def __init__(self,state_dims,action_dims):\n",
    "    super(Critic,self).__init__()\n",
    "# First Critic Network\n",
    "    self.layer1=nn.Linear(state_dims+action_dims,400)\n",
    "    self.layer2=nn.Linear(400,300)\n",
    "    self.layer3=nn.Linear(300,action_dims)\n",
    "# Second Critic Network\n",
    "    self.layer4=nn.Linear(state_dims+action_dims,400)\n",
    "    self.layer5=nn.Linear(400,300)\n",
    "    self.layer6=nn.Linear(300,action_dims)\n",
    "\n",
    "  def forward(self,x,u):     # x - state , u-action \n",
    "    xu=torch.cat([x,u],1)    # 1 for vertical concatenation #0 for Horizontal concatenation\n",
    "    \n",
    "# Forward propagation for first critic\n",
    "    xu=F.relu(self.layer1(xu))\n",
    "    x1=F.relu(self.layer2(x1))\n",
    "    x1=self.layer3(x1)\n",
    "    \n",
    "# Forward propagation for second critic\n",
    "    x2=F.relu(self.layer4(xu))\n",
    "    x2=F.relu(self.layer5(x2))\n",
    "    x2=self.layer6(x2)\n",
    "    return x1,x2\n",
    "\n",
    "# **********************************************************************************************************\n",
    "# Now let's look at the below network, Here we are planning to take the first critic to train the actor\n",
    "# It depends on the programmer , we can take critic2 or even the average . It doesn't matter in long run \n",
    "# **********************************************************************************************************\n",
    "\n",
    "\n",
    "  def Q1(self,x,u):        # x - state , u-action , This is used for updating Q values \n",
    "    xu=torch.cat([x,u],1)  # 1 for vertical concatenation #0 for Horizontal concatenation\n",
    "\n",
    "    xu=F.relu(self.layer1(xu))\n",
    "    x1=F.relu(self.layer2(x1))\n",
    "    x1=self.layer3(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process . \n",
    "Create a T3D class, initialize variables and get ready for step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dLeapzd3flQ"
   },
   "outputs": [],
   "source": [
    "# select the device (CPU or GPU)\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Building the whole training process into a class\n",
    "class T3D(object):\n",
    "  #making sure our T3D class can work with any environment\n",
    "#**************************************************************************************************************************\n",
    "# 1. State dims  - State parameters  : exactly where you are, what the car velocity , accelation ,distance travelled etc\n",
    "# 2. action_dims - How many actions can be taken\n",
    "# 3. max_action  - Not required as it's going to come from \n",
    "#\n",
    "#**************************************************************************************************************************\n",
    "  def __init__(self,state_dims,action_dims,max_action):\n",
    "        # Actor model we are defining and sending it to cuda\n",
    "    self.actor=Actor(state_dims,action_dims,max_action).to(device)\n",
    "    self.actor_target=Actor(state_dims,action_dims,max_action).to(device) # polyak averaging \n",
    "    \n",
    "    # load the target weights to the state dictionary. First time the weights of actor model and actor target are going to \n",
    "    # be the same \n",
    "        self.actor_target.load_state_dict(self.actor.state_dict)\n",
    "    \n",
    "    # initializing with model weights to keep them same , Optimizer is Adam here \n",
    "    self.actor.optimizer=torch.optim.Adam(self.actor.parameters())\n",
    "\n",
    "    self.critic=Critic(state_dims,action_dims).to(device)            #GD\n",
    "    self.critic_target=Critic(state_dims,action_dims).to(device)     # Polyak averaging \n",
    "    self.critic_target.load_state_dict(self.critic.state_dict)\n",
    "    \n",
    "    # initializing with model weights to keep them same \n",
    "    self.critic.optimizer=torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action=max_action\n",
    "\n",
    "#*********************************************************************************************************\n",
    "# Reshape the state variable (GPU)\n",
    "# call the actor forward function using the state variable , sent that to cpu, extract the data ,convert \n",
    "# to numpy and flatten the output \n",
    "#*********************************************************************************************************\n",
    "  def select_action(self,state):\n",
    "    state=torch.Tensor(state.reshape(1,-1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "#STEP 4\n",
    "\n",
    "#*********************************************************************************************************\n",
    "'''\n",
    "inputs to the function \n",
    "------------------------\n",
    "1. replay buffer object\n",
    "2. how many iterations should this network run \n",
    "3. batch size \n",
    "4. discount factor for bellman equation \n",
    "5. tau - for polyak averaging \n",
    "6. policy noise - the noise to add to our action (gaussian noise)\n",
    "7. noise clip - maximum allowable action  \n",
    "8. policy frequency - how often we are going to update our actor \n",
    "\n",
    "for each iteration\n",
    "1. Select sample data from replay buffer \n",
    "2. send each value to the GPU\n",
    "'''\n",
    "#*********************************************************************************************************\n",
    "\n",
    "  def train(self,replay_buffer,iterations,batch_size=100,discount=0.99,tau=0.005,policy_noise=0.2,noise_clip=0.5,policy_freq=2):\n",
    "    for it in range(iterations):\n",
    "      batch_state,batch_next_state,batch_action,batch_reward,batch_dones=replay_buffer.sample(batch_size)\n",
    "      state=torch.Tensor(batch_state).to(device)\n",
    "      next_state=torch.Tensor(batch_next_state).to(device)\n",
    "      action=torch.Tensor(batch_action).to(device)\n",
    "      reward=torch.Tensor(batch_reward).to(device)\n",
    "      done=torch.Tensor(batch_dones).to(device)\n",
    "\n",
    "#From the next state s', the actor target plays the next action a'. This is required as the next state and \n",
    "# action goes for critic. But before we send it to critic we need to add gaussian noise \n",
    "#STEP 5\n",
    "    next_action=self.actor_target.forward(next_state)\n",
    "    \n",
    "#************************************************************************************************************************\n",
    "# We add gaussian noise to this next action a' and we clamp it in a range of values supported by the environment\n",
    "# We get batch action from the replay buffer, get the data and add noise to it (0 - mean , policy_noise is the deviation ) \n",
    "# clamp the noise with min and max value \n",
    "# So next action is next action + noise which is clamped with the max action value from -ve to +ve\n",
    "#************************************************************************************************************************\n",
    "\n",
    "#STEP 6\n",
    "\n",
    "    noise=torch.Tensor(batch_action).data.normal_(0,policy_noise).to(device)\n",
    "    noise=noise.clamp(-noise_clip,noise_clip)\n",
    "    next_action=(next_action+noise).clamp(-self.max_action,self.max_action)\n",
    "    \n",
    "\n",
    "#STEP 7\n",
    "#************************************************************************************************************************\n",
    "#  Now the two critic targets take each the tuple (s',a') as input and return \n",
    "#  two Q values , Qt1(s',a') and Qt2(s',a') as outputs\n",
    "#************************************************************************************************************************\n",
    "\n",
    "    target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "#STEP 8\n",
    "# Keep the minimum of the target Q values \n",
    "\n",
    "    target_Q = torch.min(target_Q1, target_Q2)\n",
    "    \n",
    "#STEP 9\n",
    "#************************************************************************************************************************\n",
    "'''\n",
    "We get the final target of the two critic models, which is \n",
    " Qt = r + gamma * min(Qt1,Qt2)\n",
    " we can define \n",
    " target_q =  reward + discount * torch.min(Qt1,Qt2)\n",
    "but it won't work \n",
    "First, we are only supposed to run this if the episode is over , which means we need to integrate Done \n",
    "Second , target_q would create it's BP/communication graph, and without detaching Qt1/Qt2 from their own graph ,\n",
    "We are complicating things, i:e we need to use detach.\n",
    "'''\n",
    "#************************************************************************************************************************\n",
    "'''\n",
    "# target_Q = reward + (1-done) * discount * target_Q\n",
    "# 0 = episode not over , 1 - episode over \n",
    "# We can't run the above equation efficiently as some components are in computational graphs and some are not.\n",
    "# So we need to make one minor modifications \n",
    "# As target_Q1 and target_Q2 are from different computation chaims/maps. Hence it's very much required to detach \n",
    "# before we compute the target_Q again . \n",
    "'''\n",
    "#************************************************************************************************************************\n",
    "\n",
    "    target_Q = reward + ((1-done) *discount * target_Q).detach()\n",
    "\n",
    "# STEP 10\n",
    "#\n",
    "\n",
    "#Two critic models take (s,a) and return the two Q values \n",
    "\n",
    "    current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "# STEP 11 - We compute the loss coming from the two critic models. The critic loss is the sum of both crictic loss1 and \n",
    "# critic loss 2 \n",
    "\n",
    "    critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "# STEP 12 Backpropagate this critic loss and update the parameters of two critic models \n",
    "# models with a Adam optimizer  \n",
    "\n",
    "    self.critic_optimizer.zero_grad()    # initialize the gradients to zero\n",
    "    critic_loss.backward()               # computing the gradients \n",
    "    self.critic_optimizer.step()         # performing the weight updates \n",
    "    \n",
    "# STEP 13\n",
    "# Once every 2 iterations , we update our Actor model by performing gradient ASCENT on the output of the first Critic model\n",
    "'''\n",
    "This is how it happens .\n",
    "1. Actor take the next state and predicts the next action \n",
    "2. The next state and next action goes to both the critic target models \n",
    "3. Then we need to take the minimum of the above\n",
    "4. The minimum goes to both the critic models \n",
    "5. Then it's going to minimize the loss \n",
    "The process from 1-5 repeats twice and then \n",
    "6. it updates the actor model once \n",
    "\n",
    "Now this whole process 1-6 runs twice beofre it do the polyak averaging for the critic targets. That means the updates \n",
    "would have happened 4 times \n",
    "\n",
    "'''\n",
    "    # Once every two iterations , we update our Actor model by performing gradient ascent on the output \n",
    "    # of the first Critic model \n",
    "    \n",
    "    if it % policy_freq==0:\n",
    "      # this is DPG part  \n",
    "     # the Actor takes state gives action , the state and action is given to critic, then it takes the mean  \n",
    "      actor_loss = -self.critic.Q1(state, self.actor(state)).mean()  \n",
    "      self.actor_optimizer.zero_grad()      # Optimizer \n",
    "      actor_loss.backward()                 # Backpropagation\n",
    "      self.actor_optimizer.step()           # Optimizes the step \n",
    "        \n",
    "        \n",
    "#STEP 14\n",
    "#********************************************************************************************************************\n",
    "# Still , in once every two iterations , we update our Actor Target by polyak Averaging \n",
    "# The above model should run twice that means the critic model would have updated 4 times \n",
    "# For every combination of actor parameter and actor target parameter we take the param.data and target.param.data \n",
    "# and perform polyak averaging to update target_param data \n",
    "#********************************************************************************************************************\n",
    "\n",
    "   for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "            \n",
    "            \n",
    "# STEP 15\n",
    "#********************************************************************************************************************\n",
    "# Still , in once every two iterations , we update our Critic Target by polyak Averaging \n",
    "# For every combination of critic parameter and critic target parameter we take the param.data and target.param.data \n",
    "# and perform polyak averaging to update target_param data \n",
    "#********************************************************************************************************************\n",
    "\n",
    "   for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "t3d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
